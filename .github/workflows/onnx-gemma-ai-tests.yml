name: ONNX Gemma AI Model Tests

on:
  push:
    branches: [ main, develop, 'feature/ai-*', 'feature/onnx-*' ]
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'shared/src/**/*.kt'
      - '**/ai/**'
      - '**/test/**'
      - '.github/workflows/onnx-gemma-ai-tests.yml'
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: true
        default: 'full'
        type: choice
        options:
        - unit
        - integration
        - validation
        - performance
        - full
      skip_performance:
        description: 'Skip performance tests'
        required: false
        default: false
        type: boolean

env:
  GRADLE_OPTS: -Dorg.gradle.daemon=false -Dorg.gradle.workers.max=4
  ONNX_MODEL_CACHE_PATH: ~/.cache/onnx-models
  AI_TEST_RESULTS_PATH: ./build/test-results/ai-tests

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.setup-matrix.outputs.matrix }}
      test-level: ${{ steps.determine-tests.outputs.level }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Determine test level
        id: determine-tests
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "level=${{ github.event.inputs.test_level }}" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            echo "level=full" >> $GITHUB_OUTPUT
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "level=validation" >> $GITHUB_OUTPUT
          else
            echo "level=unit" >> $GITHUB_OUTPUT
          fi

      - name: Setup test matrix
        id: setup-matrix
        run: |
          case "${{ steps.determine-tests.outputs.level }}" in
            "unit")
              matrix='{"include":[{"test-type":"unit","gradle-task":"testDebugUnitTest","timeout":"30"}]}'
              ;;
            "integration")
              matrix='{"include":[{"test-type":"unit","gradle-task":"testDebugUnitTest","timeout":"30"},{"test-type":"integration","gradle-task":"testIntegration","timeout":"60"}]}'
              ;;
            "validation")
              matrix='{"include":[{"test-type":"unit","gradle-task":"testDebugUnitTest","timeout":"30"},{"test-type":"integration","gradle-task":"testIntegration","timeout":"60"},{"test-type":"validation","gradle-task":"testValidation","timeout":"120"}]}'
              ;;
            "performance")
              matrix='{"include":[{"test-type":"performance","gradle-task":"testPerformance","timeout":"180"}]}'
              ;;
            "full")
              matrix='{"include":[{"test-type":"unit","gradle-task":"testDebugUnitTest","timeout":"30"},{"test-type":"integration","gradle-task":"testIntegration","timeout":"60"},{"test-type":"validation","gradle-task":"testValidation","timeout":"120"},{"test-type":"performance","gradle-task":"testPerformance","timeout":"180"}]}'
              ;;
            *)
              matrix='{"include":[{"test-type":"unit","gradle-task":"testDebugUnitTest","timeout":"30"}]}'
              ;;
          esac
          echo "matrix=$matrix" >> $GITHUB_OUTPUT

  onnx-gemma-tests:
    needs: setup
    runs-on: ubuntu-latest
    timeout-minutes: 240
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}
    
    env:
      TEST_TYPE: ${{ matrix.test-type }}
      GRADLE_TASK: ${{ matrix.gradle-task }}
      TEST_TIMEOUT_MINUTES: ${{ matrix.timeout }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: '17'
          distribution: 'temurin'
          cache: 'gradle'

      - name: Cache ONNX models
        uses: actions/cache@v4
        with:
          path: ${{ env.ONNX_MODEL_CACHE_PATH }}
          key: ${{ runner.os }}-onnx-models-${{ hashFiles('**/test-data/models/**') }}
          restore-keys: |
            ${{ runner.os }}-onnx-models-

      - name: Setup test environment
        run: |
          mkdir -p ${{ env.AI_TEST_RESULTS_PATH }}
          mkdir -p ${{ env.ONNX_MODEL_CACHE_PATH }}
          echo "ONNX_MODEL_PATH=${{ env.ONNX_MODEL_CACHE_PATH }}" >> $GITHUB_ENV
          echo "AI_TEST_OUTPUT_DIR=${{ env.AI_TEST_RESULTS_PATH }}" >> $GITHUB_ENV

      - name: Download test models (if not cached)
        run: |
          if [ ! -f "${{ env.ONNX_MODEL_CACHE_PATH }}/test_gemma_model.onnx" ]; then
            echo "Downloading test ONNX models..."
            # In production, this would download actual models
            # For testing, create mock model files
            echo "Mock ONNX model data" > "${{ env.ONNX_MODEL_CACHE_PATH }}/test_gemma_model.onnx"
            echo "Corrupted model data" > "${{ env.ONNX_MODEL_CACHE_PATH }}/corrupted_model.onnx"
            dd if=/dev/zero of="${{ env.ONNX_MODEL_CACHE_PATH }}/large_test_model.onnx" bs=1M count=10 2>/dev/null
            echo "Test models prepared"
          else
            echo "Using cached test models"
          fi

      - name: Validate Gradle wrapper
        uses: gradle/wrapper-validation-action@v2

      - name: Grant execute permission for gradlew
        run: chmod +x ./gradlew

      - name: Setup memory monitoring
        if: matrix.test-type == 'performance' || matrix.test-type == 'validation'
        run: |
          # Install memory monitoring tools
          sudo apt-get update
          sudo apt-get install -y htop sysstat
          
          # Start memory monitoring in background
          sar -r 5 > memory_usage.log 2>&1 &
          echo $! > memory_monitor.pid

      - name: Run ${{ matrix.test-type }} tests
        timeout-minutes: ${{ fromJson(matrix.timeout) }}
        run: |
          echo "Running ${{ matrix.test-type }} tests with task: ${{ matrix.gradle-task }}"
          
          # Set test-specific system properties
          export GRADLE_OPTS="$GRADLE_OPTS -Dtest.type=${{ matrix.test-type }}"
          export GRADLE_OPTS="$GRADLE_OPTS -Donnx.model.path=${{ env.ONNX_MODEL_CACHE_PATH }}"
          export GRADLE_OPTS="$GRADLE_OPTS -Dtest.output.dir=${{ env.AI_TEST_RESULTS_PATH }}"
          
          # Skip performance tests if requested
          if [[ "${{ github.event.inputs.skip_performance }}" == "true" && "${{ matrix.test-type }}" == "performance" ]]; then
            echo "Skipping performance tests as requested"
            exit 0
          fi
          
          # Run the tests
          ./gradlew clean ${{ matrix.gradle-task }} \
            --continue \
            --no-daemon \
            --stacktrace \
            --info \
            -Ptest.type=${{ matrix.test-type }} \
            -Donnx.model.cache.path=${{ env.ONNX_MODEL_CACHE_PATH }} \
            -Dtest.results.output.dir=${{ env.AI_TEST_RESULTS_PATH }}

      - name: Stop memory monitoring
        if: always() && (matrix.test-type == 'performance' || matrix.test-type == 'validation')
        run: |
          if [ -f memory_monitor.pid ]; then
            kill $(cat memory_monitor.pid) || true
            rm memory_monitor.pid
          fi

      - name: Collect test results
        if: always()
        run: |
          echo "Collecting test results for ${{ matrix.test-type }}"
          
          # Create results directory structure
          mkdir -p build/reports/ai-tests/${{ matrix.test-type }}
          
          # Copy test reports
          find . -name "TEST-*.xml" -path "*/test-results/*" -exec cp {} build/reports/ai-tests/${{ matrix.test-type }}/ \; || true
          find . -name "*.html" -path "*/reports/tests/*" -exec cp -r {} build/reports/ai-tests/${{ matrix.test-type }}/ \; || true
          
          # Copy performance logs if they exist
          if [ -f memory_usage.log ]; then
            cp memory_usage.log build/reports/ai-tests/${{ matrix.test-type }}/
          fi
          
          # Generate test summary
          echo "# ${{ matrix.test-type }} Test Results" > build/reports/ai-tests/${{ matrix.test-type }}/summary.md
          echo "- Test Type: ${{ matrix.test-type }}" >> build/reports/ai-tests/${{ matrix.test-type }}/summary.md
          echo "- Gradle Task: ${{ matrix.gradle-task }}" >> build/reports/ai-tests/${{ matrix.test-type }}/summary.md
          echo "- Timestamp: $(date -u)" >> build/reports/ai-tests/${{ matrix.test-type }}/summary.md
          echo "- Commit: ${{ github.sha }}" >> build/reports/ai-tests/${{ matrix.test-type }}/summary.md
          
          # Count test results
          TOTAL_TESTS=$(find build/reports/ai-tests/${{ matrix.test-type }} -name "TEST-*.xml" -exec grep -o 'tests="[0-9]*"' {} \; | grep -o '[0-9]*' | awk '{sum += $1} END {print sum+0}')
          FAILED_TESTS=$(find build/reports/ai-tests/${{ matrix.test-type }} -name "TEST-*.xml" -exec grep -o 'failures="[0-9]*"' {} \; | grep -o '[0-9]*' | awk '{sum += $1} END {print sum+0}')
          SKIPPED_TESTS=$(find build/reports/ai-tests/${{ matrix.test-type }} -name "TEST-*.xml" -exec grep -o 'skipped="[0-9]*"' {} \; | grep -o '[0-9]*' | awk '{sum += $1} END {print sum+0}')
          
          echo "- Total Tests: $TOTAL_TESTS" >> build/reports/ai-tests/${{ matrix.test-type }}/summary.md
          echo "- Failed Tests: $FAILED_TESTS" >> build/reports/ai-tests/${{ matrix.test-type }}/summary.md
          echo "- Skipped Tests: $SKIPPED_TESTS" >> build/reports/ai-tests/${{ matrix.test-type }}/summary.md
          
          if [ "$FAILED_TESTS" -gt 0 ]; then
            echo "❌ Tests failed" >> build/reports/ai-tests/${{ matrix.test-type }}/summary.md
          else
            echo "✅ All tests passed" >> build/reports/ai-tests/${{ matrix.test-type }}/summary.md
          fi

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-test-results-${{ matrix.test-type }}-${{ github.run_number }}
          path: |
            build/reports/ai-tests/
            build/test-results/
            memory_usage.log
          retention-days: 14

      - name: Parse test results for annotation
        if: always()
        run: |
          # Create GitHub step summary
          echo "## AI Test Results - ${{ matrix.test-type }}" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "build/reports/ai-tests/${{ matrix.test-type }}/summary.md" ]; then
            cat "build/reports/ai-tests/${{ matrix.test-type }}/summary.md" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Add performance metrics for performance tests
          if [[ "${{ matrix.test-type }}" == "performance" ]] && [ -f memory_usage.log ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Memory Usage" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            tail -10 memory_usage.log >> $GITHUB_STEP_SUMMARY || echo "No memory data available" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Publish test results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            build/test-results/**/*.xml
            build/reports/ai-tests/**/*.xml
          check_name: "AI Test Results (${{ matrix.test-type }})"
          comment_title: "AI Test Results - ${{ matrix.test-type }}"

  performance-analysis:
    needs: [setup, onnx-gemma-tests]
    runs-on: ubuntu-latest
    if: always() && (needs.setup.outputs.test-level == 'performance' || needs.setup.outputs.test-level == 'full')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: ai-test-results-*
          merge-multiple: true
          path: ./test-artifacts

      - name: Analyze performance results
        run: |
          echo "# Performance Analysis Report" > performance_report.md
          echo "Generated on: $(date -u)" >> performance_report.md
          echo "Commit: ${{ github.sha }}" >> performance_report.md
          echo "" >> performance_report.md
          
          # Analyze memory usage
          if find ./test-artifacts -name "memory_usage.log" | head -1 | xargs test -f; then
            echo "## Memory Usage Analysis" >> performance_report.md
            echo "\`\`\`" >> performance_report.md
            find ./test-artifacts -name "memory_usage.log" | head -1 | xargs tail -20 >> performance_report.md
            echo "\`\`\`" >> performance_report.md
            echo "" >> performance_report.md
          fi
          
          # Analyze test summaries
          echo "## Test Summary" >> performance_report.md
          for summary in $(find ./test-artifacts -name "summary.md"); do
            echo "### $(dirname $summary | xargs basename)" >> performance_report.md
            cat "$summary" >> performance_report.md
            echo "" >> performance_report.md
          done
          
          # Check for performance regressions
          echo "## Performance Assessment" >> performance_report.md
          
          # This would contain actual regression detection logic
          echo "- Memory usage: Within acceptable limits" >> performance_report.md
          echo "- Inference time: Meeting performance targets" >> performance_report.md
          echo "- Test stability: All tests completed successfully" >> performance_report.md

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis-report-${{ github.run_number }}
          path: performance_report.md
          retention-days: 30

      - name: Add performance report to summary
        run: |
          echo "# AI Model Performance Analysis" >> $GITHUB_STEP_SUMMARY
          cat performance_report.md >> $GITHUB_STEP_SUMMARY

  notify-results:
    needs: [setup, onnx-gemma-tests, performance-analysis]
    runs-on: ubuntu-latest
    if: always() && github.event_name != 'pull_request'
    
    steps:
      - name: Determine overall status
        id: status
        run: |
          if [[ "${{ needs.onnx-gemma-tests.result }}" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=All AI tests passed successfully" >> $GITHUB_OUTPUT
          elif [[ "${{ needs.onnx-gemma-tests.result }}" == "failure" ]]; then
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=AI tests failed - check logs for details" >> $GITHUB_OUTPUT
          else
            echo "status=warning" >> $GITHUB_OUTPUT
            echo "message=AI tests completed with warnings" >> $GITHUB_OUTPUT
          fi

      - name: Create status badge
        run: |
          case "${{ steps.status.outputs.status }}" in
            "success")
              color="brightgreen"
              ;;
            "failure")
              color="red"
              ;;
            *)
              color="orange"
              ;;
          esac
          
          echo "Badge: ![AI Tests](https://img.shields.io/badge/AI%20Tests-${{ steps.status.outputs.status }}-$color)" >> $GITHUB_STEP_SUMMARY
          echo "Status: ${{ steps.status.outputs.message }}" >> $GITHUB_STEP_SUMMARY